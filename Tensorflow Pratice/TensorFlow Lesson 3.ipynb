{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69a0693f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "sentences = [\n",
    "    'I love my dog',\n",
    "    'I, love my cat',\n",
    "    'You love my dog!'\n",
    "]\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=100)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62081c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
      "[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n",
      "[[ 0  5  3  2  4]\n",
      " [ 0  5  3  2  7]\n",
      " [ 0  6  3  2  4]\n",
      " [ 9  2  4 10 11]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "sentences = [\n",
    "    'I love my dog',\n",
    "    'I love my cat',\n",
    "    'You love my dog!',\n",
    "    'Do you think my dog is amazing?'\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer(num_words=100, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "padded = pad_sequences(sequences, maxlen=5)\n",
    "print(word_index)\n",
    "print(sequences)\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90790231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]\n",
      "[[0 0 0 0 0 5 1 3 2 4]\n",
      " [0 0 0 0 0 2 4 1 2 1]]\n"
     ]
    }
   ],
   "source": [
    "test_data = [\n",
    "    'i really love my dog',\n",
    "    'my dog loves my manatee'\n",
    "]\n",
    "test_sequence = tokenizer.texts_to_sequences(test_data)\n",
    "padded_test = pad_sequences(test_sequence, maxlen=10)\n",
    "print(test_sequence)\n",
    "print(padded_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5816ccba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1dRzdJS7-cZS4S5CuUc32MZYSLJGkkxnp\n",
      "To: D:\\projects\\Tensorflow Certification\\sarcasm.json\n",
      "\n",
      "0.00B [00:00, ?B/s]\n",
      "524kB [00:01, 364kB/s]\n",
      "1.05MB [00:01, 799kB/s]\n",
      "1.57MB [00:02, 863kB/s]\n",
      "2.10MB [00:02, 1.14MB/s]\n",
      "2.62MB [00:02, 1.24MB/s]\n",
      "3.15MB [00:02, 1.54MB/s]\n",
      "3.67MB [00:03, 1.51MB/s]\n",
      "4.19MB [00:03, 1.95MB/s]\n",
      "4.72MB [00:03, 2.10MB/s]\n",
      "5.24MB [00:03, 2.09MB/s]\n",
      "5.64MB [00:03, 2.12MB/s]\n",
      "5.64MB [00:03, 1.42MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29657\n",
      "[4, 8435, 3338, 2746, 22, 2, 166, 8436, 416, 3112, 6, 258, 9, 1002]\n",
      "[   4 8435 3338 2746   22    2  166 8436  416 3112    6  258    9 1002\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "(26709, 40)\n"
     ]
    }
   ],
   "source": [
    "!gdown --id 1dRzdJS7-cZS4S5CuUc32MZYSLJGkkxnp\n",
    "\n",
    "import json \n",
    "\n",
    "with open('sarcasm.json','r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "sentences = []\n",
    "labels = []\n",
    "urls = []\n",
    "\n",
    "for row in data:\n",
    "    sentences.append(row['headline'])\n",
    "    labels.append(row['is_sarcastic'])\n",
    "    urls.append(row['article_link'])\n",
    "\n",
    "tokenizer = Tokenizer(oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "padded = pad_sequences(sequences,padding='post')\n",
    "print(len(word_index))\n",
    "print(sequences[1])\n",
    "print(padded[1])\n",
    "print(padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "665c4d86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1rX10xeI3eUJmOLsc4pOPY6AnCLO8DxNj\n",
      "To: D:\\projects\\Tensorflow Certification\\bbc-text.csv\n",
      "\n",
      "0.00B [00:00, ?B/s]\n",
      "524kB [00:00, 1.29MB/s]\n",
      "1.05MB [00:00, 1.40MB/s]\n",
      "1.57MB [00:01, 1.59MB/s]\n",
      "2.10MB [00:01, 1.87MB/s]\n",
      "2.62MB [00:01, 2.12MB/s]\n",
      "3.15MB [00:01, 2.36MB/s]\n",
      "3.67MB [00:01, 2.48MB/s]\n",
      "4.19MB [00:01, 2.79MB/s]\n",
      "4.72MB [00:02, 2.89MB/s]\n",
      "5.06MB [00:02, 2.75MB/s]\n",
      "5.06MB [00:02, 2.26MB/s]\n"
     ]
    }
   ],
   "source": [
    "!gdown --id 1rX10xeI3eUJmOLsc4pOPY6AnCLO8DxNj\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ca822c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "590d5c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2225\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "labels = []\n",
    "with open('bbc-text.csv','r') as f:\n",
    "    data = csv.reader(f, delimiter = ',')\n",
    "    next(data, None)\n",
    "    for row in data:\n",
    "        labels.append(row[0])\n",
    "        sentence = row[1]\n",
    "        for word in stopwords:\n",
    "            token = \" \" + word + \" \"\n",
    "            sentence = sentence.replace(token,' ')\n",
    "            sentence = sentence.replace('  ', ' ')\n",
    "        sentences.append(sentence)\n",
    "        \n",
    "print(len(sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f420ed73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29714\n",
      "[  96  176 1158 ...    0    0    0]\n",
      "(2225, 2442)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "padded = pad_sequences(sequences, padding='post')\n",
    "\n",
    "print(len(word_index))\n",
    "print(padded[0])\n",
    "print(padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c89bfb4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sport': 1, 'business': 2, 'politics': 3, 'tech': 4, 'entertainment': 5}\n"
     ]
    }
   ],
   "source": [
    "label_tokenizer = Tokenizer()\n",
    "label_tokenizer.fit_on_texts(labels)\n",
    "label_word_index = label_tokenizer.word_index\n",
    "label_sequences = label_tokenizer.texts_to_sequences(labels)\n",
    "#print(label_sequences)\n",
    "print(label_word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f15a892",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
